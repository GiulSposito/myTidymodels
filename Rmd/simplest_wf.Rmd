---
title: "Simplest ML Workflow"
subtitle: "using tidymodel"
author: "Giuliano Sposito"
output:
  md_document:
    variant: markdown_github
---

# Intro

This document emulates the simplest steps to make a straightforward ML using [`{tidyverse}`](https://www.tidymodels.org/) package. These are the steps:

1. use [`{rsample}`](https://rsample.tidymodels.org/) to split the dataset between training and testing subsets
1. use [`{recipe}`](https://recipes.tidymodels.org/) to make some data preprocessing script
1. use [`{parnsip}`](https://parsnip.tidymodels.org/) to define a **ranger random forest** model
1. put the recipe and the model in a [`{workflow}`](https://workflows.tidymodels.org/) object
1. fit a model using the training subset
1. use the fitted model to make a prediction
1. use [`{yardstick}`](https://yardstick.tidymodels.org/) the check the model performance

# Packages

```{r setup, warning=FALSE, message=FALSE}
library(tidymodels)  
library(mlbench)    # mlbench is a library with several dataset to perform ML trainig
library(skimr)      # to look the dataset

# loading "Boston Housing" dataset
data("BostonHousing")
```

# Dataset: Boston Housing Dataset

Housing data contains 506 census tracts of Boston from the 1970 census. The dataframe BostonHousing contains the original data by Harrison and Rubinfeld (1979), the dataframe BostonHousing2 the corrected version with additional spatial information.

You can include this data by installing mlbench library or download the dataset. The data has following features, medv being the target variable:

+ crim - per capita crime rate by town
+ zn - proportion of residential land zoned for lots over 25,000 sq.ft
+ indus - proportion of non-retail business acres per town
+ chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
+ nox - nitric oxides concentration (parts per 10 million)
+ rm - average number of rooms per dwelling
+ age - proportion of owner-occupied units built prior to 1940
+ dis - weighted distances to five Boston employment centres
+ rad - index of accessibility to radial highways
+ tax - full-value property-tax rate per USD 10,000
+ ptratio- pupil-teacher ratio by town
+ b 1000(B - 0.63)^2, where B is the proportion of blacks by town
+ lstat - percentage of lower status of the population
+ medv - median value of owner-occupied homes in USD 1000â€™s

```{r dataoverview}
BostonHousing %>% 
  skim()
```

We'll try to predic **medv** (median value of owner-occupied homes).

# Training & Testing Datasets

```{r split}
boston_split <- initial_split(BostonHousing)
boston_split
```

# Data Preprocessing

```{r preprocRecipe}
recp <- BostonHousing %>% 
  recipe(medv~.) %>%                               # formula goes here
  step_nzv(all_predictors(), -all_nominal()) %>%   # remove near zero var
  step_center(all_predictors(),-all_nominal()) %>% # center 
  step_scale(all_predictors(),-all_nominal()) %>%  # scale
  step_BoxCox(all_predictors(), -all_nominal())    # box cox normalization
recp
```

# Model Specification

```{r model}
model_eng <- rand_forest(mode="regression") %>% 
  set_engine("ranger")
model_eng
```

# Workflow

```{r wf}
wf <- workflow() %>% 
  add_recipe(recp) %>%  # preprocessing specifiation (with formula)
  add_model(model_eng)  # model specification
wf
```

# Training the model

```{r}
# the workflow do all by itself
# calculates the data preprocessing (recipe)
# apply to the training set
# fit the model using it
model_fit <- fit(wf, training(boston_split)) 
model_fit
```

# Predict

```{r}
# the prediction applied on the fitted workflow automatically
# apply the trained transformation on the new dataset
# and predict the output using the trained model
y_hat <- predict(model_fit, testing(boston_split))
head(y_hat)
```

# Evaluate

```{r}
y_hat %>% 
  bind_cols(testing(boston_split)) %>% # binds the "true value"
  metrics(truth=medv, estimate=.pred)  # get the estimation metrics (automatically)
```

