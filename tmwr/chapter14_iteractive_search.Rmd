---
title: "Interactive Search"
output: 
  md_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

# Iteractive Search

> When grid search is infeasible or inefficient, iterative methods are a sensible approach for optimizing tuning parameters.

We once again use the cell segmentation data, for modeling, with a support vector machine (SVM) model to demonstrate sequential tuning methods. The two tuning parameters to optimize are the SVM cost value and the radial basis function kernel parameter $\sigma$. Both parameters can have a profound effect on the model complexity and performance.

The SVM model uses a dot product and, for this reason, it is necessary to center and scale the predictors. Like the multilayer perceptron model, this model would benefit from the use of PCA feature extraction. However, we will not use this third tuning parameter in this chapter so that we can visualize the search process in two dimensions.

```{r}
# put rnotbook in the same workdir
knitr::opts_knit$set(root.dir = normalizePath(rprojroot::find_rstudio_root_file())) 
library(tidymodels)

data(cells)
# removing the default 
cells <- cells |> select(-case)
cell_folds <- vfold_cv(cells, 5)

# which metric will be used?
roc_res <- metric_set(roc_auc)

svm_rec <- 
  recipe(class ~., data=cells) |> 
  step_YeoJohnson(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors())

svm_spec <-
  svm_rbf(cost=tune(), rbf_sigma = tune()) |> 
  set_engine("kernlab") |> 
  set_mode("classification")

svm_wflow <- 
  workflow() |> 
  add_model(svm_spec) |> 
  add_recipe(svm_rec) 

svm_rec
svm_spec
svm_wflow
```

The defult parameters ranges, for the two tunning parameters `cost` and  `rbf_sigma` are:

```{r}
cost()
rbf_sigma()
```

For illustration, let’s slightly change the kernel parameter range, to improve the visualizations of the search:

```{r}
svm_param <- 
  svm_wflow |> 
  extract_parameter_set_dials() |> 
  update(rbf_sigma = rbf_sigma(c(-7, -1)))
```


```{r cache=TRUE, eval=FALSE}
set.seed(1975)
library(doMC)
registerDoMC(cores=parallel::detectCores()-1)
svm_tune_res <- svm_wflow |> 
  tune_grid(
    cell_folds, 
    grid = svm_param |> grid_regular(levels=35),
    metrics=roc_res
  )

saveRDS(svm_tune_res, "./svm_tune_res.rds")
```



```{r}
svm_tune_res <- readRDS("./svm_tune_res.rds")

best_tune <- svm_tune_res |>
  select_best()

svm_tune_res |>
  collect_metrics() |>
  ggplot() +
  geom_tile(aes(rbf_sigma, cost, fill = mean)) +
  geom_point(
    data = best_tune,
    mapping = aes(rbf_sigma, cost),
    color = "black"
  ) +
  scale_x_log10() +
  scale_y_log10() +
  coord_fixed() +
  labs(title = "ROC AUC Surface", x = "sigma") +
  theme_minimal()

```

There is a large swath in the lower diagonal of the parameter space that is relatively flat with poor performance. A ridge of best performance occurs in the upper-right portion of the space. The black dot indicates the best settings. The transition from the plateau of poor results to the ridge of best performance is very sharp. There is also a sharp drop in the area under the ROC curve just to the right of the ridge.


```{r, cache=TRUE}
set.seed(1401)
start_grid <- 
  svm_param %>% 
  update(
    cost = cost(c(-6, 1)),
    rbf_sigma = rbf_sigma(c(-6, -4))
  ) %>% 
  grid_regular(levels = 2)

set.seed(1402)
svm_initial <- 
  svm_wflow %>% 
  tune_grid(resamples = cell_folds, grid = start_grid, metrics = roc_res)

collect_metrics(svm_initial)
```

This initial grid shows fairly equivalent results, with no individual point much better than any of the others. These results can be ingested by the iterative tuning functions discussed in the following sections to be used as initial values.

## Bayesian Optimization

Bayesian optimization techniques analyze the current resampling results and create a predictive model to suggest tuning parameter values that have yet to be evaluated. The suggested parameter combination is then resampled. These results are then used in another predictive model that recommends more candidate values for testing, and so on. The process proceeds for a set number of iterations or until no further improvements occur. Shahriari et al. (2016) and Frazier (2018) are good introductions to Bayesian optimization.

When using Bayesian optimization, the primary concerns are how to create the model and how to select parameters recommended by that model. First, let’s consider the technique most commonly used for Bayesian optimization, the Gaussian process model.

## A GAUSSIAN PROCESS MODE

Mathematically, a GP is a collection of random variables whose joint probability distribution is multivariate Gaussian. In the context of our application, this is the collection of performance metrics for the tuning parameter candidate values.

These are assumed to be distributed as multivariate Gaussian. The inputs that define the independent variables/predictors for the GP model are the corresponding tuning parameter values:

```{r}
collect_metrics(svm_initial) |> 
  select(cost, rbf_sigma)
```

```{r, cache=TRUE}
ctrl <- control_bayes(verbose = T)


library(doMC)
registerDoMC(cores=parallel::detectCores()-1)
set.seed(1403)
svm_bo <- 
  svm_wflow |> 
  tune_bayes(
    resamples = cell_folds,
    metrics = roc_res,
    initial = svm_initial, 
    param_info = svm_param,
    iter=25,
    control = ctrl
  )
```

Comparing results

```{r}
# bayes process
show_best(svm_bo)
# grid search
show_best(svm_tune_res)
```

```{r}
autoplot(svm_bo, type = "performance")
```

```{r}
# gausian process search in the parameters space
svm_bo |> 
  collect_metrics() |> 
  filter(.iter>0) |> 
  ggplot() +
  geom_tile(data = collect_metrics(svm_tune_res), mapping=aes(x=rbf_sigma, y=cost, fill = mean)) +
  geom_path(aes(rbf_sigma, cost)) +
  geom_label(aes(x=rbf_sigma, y=cost,label=.iter, fill=mean)) +
  scale_x_log10() +
  scale_y_log10() +
  coord_fixed() +
  theme_minimal()

svm_bo |> 
  select_best()
```

## SIMULATED ANNEALING

Simulated annealing (SA) is a general nonlinear search routine inspired by the process in which metal cools. It is a global search method that can effectively navigate many different types of search landscapes, including discontinuous functions. Unlike most gradient-based optimization routines, simulated annealing can reassess previous solutions.

The process of using simulated annealing starts with an initial value and embarks on a controlled random walk through the parameter space. Each new candidate parameter value is a small perturbation of the previous value that keeps the new point within a local neighborhood.

> The acceptance probabilities of simulated annealing allow the search to proceed in the wrong direction, at least for the short term, with the potential to find a much better region of the parameter space in the long run.

```{r, cache=TRUE}
library(finetune)
ctrl_sa <- control_sim_anneal(verbose=T, no_improve=10L)

set.seed(1404)
library(doMC)
registerDoMC(cores=parallel::detectCores()-1)
svm_sa <- 
  svm_wflow |> 
  tune_sim_anneal(
    resamples = cell_folds, 
    metrics = roc_res,
    initial = svm_initial,
    param_info = svm_param, 
    iter = 50, 
    control = ctrl_sa
  )

```

Let`s check the results

```{r}
svm_sa |> 
  show_best()

autoplot(svm_sa, type="performance")
```

```{r}
# simulated annealing search in the parameters space
svm_sa |> 
  collect_metrics() |> 
  filter(.iter>0) |> 
  ggplot() +
  geom_tile(data = collect_metrics(svm_tune_res), mapping=aes(x=rbf_sigma, y=cost, fill = mean)) +
  geom_path(aes(rbf_sigma, cost)) +
  geom_label(aes(x=rbf_sigma, y=cost,label=.iter, fill=mean)) +
  scale_x_log10() +
  scale_y_log10() +
  coord_fixed() +
  theme_minimal()
```

# Reference

All code and text came from Max Kuhn and Julia Silge`s book [Tidy Modeling with R](https://www.tmwr.org/iterative-search).
