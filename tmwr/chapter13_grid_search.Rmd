---
title: "Grid Search"
output: 
  md_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

# GRID SEARCH

## REGULAR AND NONREGULAR GRIDS

There are two main types of grids. A *regular grid* combines each parameter (with its corresponding set of possible values) factorially, i.e., by using all combinations of the sets. Alternatively, *a nonregular grid* is one where the parameter combinations are not formed from a small set of points.

Before we look at each type in more detail, let’s consider an example model: the multilayer perceptron model (a.k.a. single layer artificial neural network). The parameters marked for tuning are:

+ the number of hidden units
+ the number of fitting epochs/iterations in model training
+ the amount of weight decay penalization

```{r}
library(tidymodels)

mlp_spec <- 
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) |> 
  set_engine("nnet", trace=0) |> # The argument trace = 0 prevents extra logging of the training process
  set_mode("classification")

mlp_spec

mlp_param <- extract_parameter_set_dials(mlp_spec)
mlp_param

mlp_param |> 
  extract_parameter_dials("hidden_units")

mlp_param |> 
  extract_parameter_dials("epochs")
```

## REGULAR GRIDS

Regular grids are combinations of separate sets of parameter values. The *tidyr* function `crossing()` is one way to create a regular grid:

```{r}
crossing(
  hidden_units = 1:3, 
  penalty = c(0.0, 0.1),
  epochs = c(100, 200)
)
```

The parameter object knows the ranges of the parameters. The *dials* package contains a set of `grid_*()` functions that take the parameter object as input to produce different types of grids

```{r}
grid_regular(mlp_param, levels=2)
```

The `levels` argument is the number of levels per parameter to create. It can also take a named vector of values:

```{r}
mlp_param |> 
  grid_regular(levels=c(hidden_units=3, penalty=2, epochs=2))
```

## IRREGULAR GRIDS

```{r}
set.seed(1301)
mlp_param |> 
  grid_random(size=100) |> # size is the number of combinations
  summary()
```

The issue with random grids is that, with small-to-medium grids, random values can result in overlapping parameter combination

```{r}
library(ggforce)
set.seed(1302)
mlp_param |> 
  grid_random(size=20, original = F) |> 
  ggplot(aes(x=.panel_x, y=.panel_y)) +
  geom_point() +
  geom_blank() +
  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag=2) +
  labs(title="Random design with 20 candidates")

```

A much better approach is to use a set of experimental designs called space-filling designs. While different design methods have slightly different goals, they generally find a configuration of points that cover the parameter space with the smallest chance of overlapping or redundant values

The dials package contains functions for Latin hypercube and maximum entropy designs.

```{r}
set.seed(1303)
mlp_param %>% 
  grid_latin_hypercube(size = 20, original = FALSE) %>% 
  ggplot(aes(x = .panel_x, y = .panel_y)) + 
  geom_point() +
  geom_blank() +
  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + 
  labs(title = "Latin Hypercube design with 20 candidates")


```

While not perfect, this Latin hypercube design spaces the points farther away from one another and allows a better exploration of the hyperparameter space.

## EVALUATING THE GRID

### Dataset

We use a classification data set to demonstrate model tuning in this and the next chapter. The data come from Hill et al. (2007), who developed an automated microscopy laboratory tool for cancer research. The data consists of 56 imaging measurements on 2019 human breast cancer cells. These predictors represent shape and intensity characteristics of different parts of the cells (e.g., the nucleus, the cell boundary, etc.). The data are included in the `modeldata` package. Let’s remove one column not needed for analysis (`case`).


```{r}
library(tidymodels)
data(cells)

skimr::skim(cells)

# case is a default Train x Test classification of the dataset
cells <- cells %>% select(-case)
```

Given the dimensions of the data, we can compute performance metrics using 10-fold cross-validation:

```{r}
set.seed(1304)
cell_folds <- vfold_cv(cells)
```

Because of the high degree of correlation between predictors, it makes sense to use PCA feature extraction to decorrelate the predictors. The number of PCA components to retain is also tuned, along with the model parameters.

Many of the predictors have skewed distributions. Since PCA is variance based, extreme values can have a detrimental effect on these calculations. To counter this, let’s add a recipe step estimating a Yeo-Johnson transformation for each predictor

## Tunning the model

```{r}
# The Yeo-Johnson transformation is very similar to the Box-Cox but does not
# require the input variables to be strictly positive
mlp_rec <- recipe( class ~ ., data=cells ) |> 
  step_YeoJohnson(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors()) |> 
  step_pca(all_numeric_predictors(), num_comp = tune()) |> 
  # While the resulting PCA components are technically on the same scale, the 
  # lower-rank components tend to have a wider range than the higher-rank 
  # components. For this reason, we normalize again to coerce the predictors
  # to have the same mean and variance.
  step_normalize(all_numeric_predictors())

mlp_rec


mlp_wflow <- 
  workflow() |> 
  add_model(mlp_spec) |> 
  add_recipe(mlp_rec)

mlp_wflow
```

Let’s create a parameter object mlp_param to adjust a few of the default ranges. 

```{r}
mlp_param <- 
  mlp_wflow |> 
  extract_parameter_set_dials() |> 
  update(
    epochs = epochs(c(15,200)),
    num_comp = num_comp(c(0,40))
  )

mlp_param
```

> In `step_pca()`, using zero PCA components is a shortcut to skip the feature extraction. In this way, the original predictors can be directly compared to the results that include PCA components.

The `tune_grid()` function is the primary function for conducting grid search, if has additional arguments related to the grid:

+ *grid*: An integer or dataframe
+ *param_info*

Otherwise, the interface to `tune_grid()` is the same as `fit_resamples()`. 

```{r}
# which metric will be used?
roc_res <- metric_set(roc_auc)

library(doMC)
registerDoMC(cores=parallel::detectCores()-1)
set.seed(1305)
mlp_reg_tune <- 
  mlp_wflow |> 
  tune_grid(
    cell_folds, 
    grid = mlp_param |> grid_regular(levels=3),
    metrics=roc_res
  )

mlp_reg_tune


```

There are high-level convenience functions we can use to understand the results. First, the autoplot() method for regular grids shows the performance profiles across tuning parameters.

```{r}
autoplot(mlp_reg_tune) +
  theme(legend.position = "top")
```

For these data, the amount of penalization has the largest impact on the area under the ROC curve. The number of epochs doesn’t appear to have a pronounced effect on performance. The change in the number of hidden units appears to matter most when the amount of regularization is low (and harms performance).

```{r}
show_best(mlp_reg_tune)
```

Based on these results, it would make sense to conduct another run of grid search with larger values of the weight decay penalty.

To use a space-filling design, either the `grid` argument can be given an integer or one of the `grid_*()` functions can produce a data frame. To evaluate the same range using a maximum entropy design with 20 candidate values:

```{r}
set.seed(1306)
mlp_sfd_tune <- 
  mlp_wflow |> 
  tune_grid(
    cell_folds,
    grid = 20,
    param_info = mlp_param,
    metrics = roc_res
  )

mlp_sfd_tune
```

```{r}
autoplot(mlp_sfd_tune)
show_best(mlp_sfd_tune)
```

## FINALIZING THE MODEL

If one of the sets of possible model parameters found via `show_best()` were an attractive final option for these data, we might wish to evaluate how well it does on the test set. However, the results of `tune_grid()` only provide the substrate to choose appropriate tuning parameters.

To fit a final model, a final set of parameter values must be determined. There are two methods to do so:

+ manually pick values that appear appropriate or
+ use a select_*() function.

For example, `select_best()` will choose the parameters with the numerically best results. Let’s go back to our regular grid results and see which one is best:

```{r}
show_best(mlp_reg_tune, metric="roc_auc")
```

To manually specify these parameters, we can create a tibble with these values and then use a finalization function to splice the values back into the workflow:

```{r}
logistic_param <- 
  tibble(
    num_comp = 0,
    epochs = 125,
    hidden_units = 1,
    penalty = 1
  )

final_mlp_wflow <- 
  mlp_wflow |> 
  finalize_workflow(logistic_param)

final_mlp_wflow
```

No more values of tune() are included in this finalized workflow. Now the model can be fit to the entire training set:

```{r}
final_mlp_fit <- 
  final_mlp_wflow |> 
  fit(cells)

final_mlp_fit

extract_fit_engine(final_mlp_fit) |> NeuralNetTools::plotnet()
```


```{r}
mlp_sdf_final_wflow <- mlp_wflow |> 
  finalize_workflow(select_best(mlp_sfd_tune))

mlp_sdf_final_wflow

final_mlp_sdf_fit <-
  mlp_sdf_final_wflow |> 
  fit(cells)

final_mlp_sdf_fit

final_mlp_sdf_fit |> 
  extract_fit_engine() |> 
  NeuralNetTools::plotnet()
   
```

## TOOLS FOR CREATING TUNING SPECIFICATIONS

The `usemodel` package cna take a dataframe and a formula, than write out R code for tuning the model.

```{r}
library(usemodels)

data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

use_xgboost( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
               Latitude + Longitude, 
             data = ames_train, 
             # verbose add comments
             verbose=T)
```

> The `usemodels` package can also be used to create model fitting code with no tuning by setting the argument `tune = FALSE`.

# Reference

All code and text came from Max Kuhn and Julia Silge`s book [Tidy Modeling with R](https://www.tmwr.org/grid-search).































