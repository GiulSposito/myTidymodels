---
title: "Model Tuning"
output: 
  md_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

# Model Tuning

## What do we optimize?

For cases where the statistical properties of the tuning parameter are tractable, common statistical properties can be used as the objective function. For example, in the case of binary logistic regression, the link function can be chosen by maximizing the likelihood or information criteria. 

> degrading the likelihood by overfitting actually improves misclassification error rate

To demonstrate, consider the classification data shown in Figure 12.1 with two predictors, two classes, and a training set of 593 data points.

```{r}
library(tidymodels)
tidymodels_prefer()

data("two_class_dat")

dat_split <- initial_split(two_class_dat)
training_set <- training(dat_split)
testing_set  <- testing(dat_split)

training_set

training_set |> 
  ggplot(aes(x=A, y=B, color=Class, shape=Class)) +
  geom_point() +
  theme_light() +
  theme(legend.position = "top")

```

For a data frame `training_set`, letâ€™s create a function to compute the different models and extract the likelihood statistics for the training set (using `broom::glance()`):

```{r}

llhood <- function(...) {
  logistic_reg() |>
    set_engine("glm", ...) |>
    fit(Class ~ ., data = training_set) |>
    glance() |>
    select(logLik)
}

bind_rows(
  llhood(),
  llhood(family = binomial(link = "probit")),
  llhood(family = binomial(link = "cloglog"))) |>
    mutate(link = c("logit", "probit", "c-log-log")) |>
    arrange(desc(logLik))
  
```

According to these results, the logistic model has the best statistical properties.

From the scale of the log-likelihood values, it is difficult to understand if these differences are important or negligible. One way of improving this analysis is to resample the statistics and separate the modeling data from the data used for performance estimation.

the `mn_log_loss() `function is used to estimate the negative log-likelihood

```{r}
set.seed(1201)
rs <- vfold_cv(training_set, repeats = 10)

# Return the individual resampled performance estimates:
lloss <- function(...) {
  perf_meas <- metric_set(roc_auc, mn_log_loss)
  logistic_reg() |> 
    set_engine("glm",...) |> 
    fit_resamples(Class~A+B, rs, metrics=perf_meas) |> 
    collect_metrics(summarize=FALSE) |> 
    select(id, id2, .metric, .estimate)
}

resampled_res <- bind_rows(
  lloss()                                |> mutate(model="logistic"), 
  lloss(family=binomial(link="probit"))  |> mutate(model="probit"),
  lloss(family=binomial(link="cloglog")) |> mutate(model="c-log-log")
) |> 
  # convert log-loss to log-likelihood
  mutate(.estimate = if_else(.metric=="mn_log_loss", -.estimate, .estimate)) |> 
  group_by(model, .metric) |> 
  summarize(
    mean = mean(.estimate, na.rm=T),
    std_err = sd(.estimate, na.rm=T) / sqrt(n()),
    .groups = "drop"
  )

resampled_res

resampled_res |> 
  filter(.metric=="mn_log_loss") |> 
  ggplot(aes(x=mean, y=model)) +
  geom_point() +
  geom_errorbar(aes(xmin=mean-1.64 * std_err, xmax=mean+1.64*std_err), width=.1) +
  labs(y=NULL, x="log-likelihood") +
  theme_light() +
  labs(title="Log-Likelihood")

```

These results exhibit evidence that the choice of the link function matters somewhat. Although there is an overlap in the confidence intervals, the logistic model has the best results.

What about a different metric? We also calculated the area under the ROC curve for each resample

```{r}
resampled_res |> 
  filter(.metric=="roc_auc") |> 
  ggplot(aes(x=mean, y=model)) +
  geom_point() +
  geom_errorbar(aes(xmin=mean-1.64 * std_err, xmax=mean+1.64*std_err), width=.1) +
  labs(y=NULL, x="roc_uac") +
  theme_light() +
  labs(title = "Area Under the ROC Curve")
```

Given the overlap of the intervals, as well as the scale of the x-axis, any of these options could be used.

```{r}

glm_models <- list(
  logit  = logistic_reg() |> set_engine("glm"),
  probit = logistic_reg() |> set_engine("glm", family=binomial(link="probit")),
  cloglog = logistic_reg() |> set_engine("glm", family=binomial(link="cloglog"))
)

wflows <- workflow_set(list(model=Class~A+B), models=glm_models)

res <- wflows |> 
  mutate(fitted = map(info,~fit(.x$workflow[[1]], training_set)),
         params = map(fitted, tidy))
res

w_res <- res |> 
  select(wflow_id, params) |> 
  unnest(params) |> 
  pivot_wider(id_cols=wflow_id, names_from=term, values_from = estimate) |> 
  janitor::clean_names()

w_res

training_set |> 
  ggplot(aes(x=A, y=B, color=Class, shape=Class)) +
  geom_point(show.legend = F) +
  geom_abline(data=w_res, aes(slope = -a/b, intercept = -intercept/b, linetype=wflow_id)) +
  coord_obs_pred() +
  theme_light() +
  theme(legend.position = "top") 
```

Remembering Sigmoid Function

$$ 

  h_{\theta} = g(z)
  
  z = \theta_0 + \theta_1x_1+\theta_2x_2
  
  \theta_0 + \theta_1x_1+\theta_2x_2 \ge 0
  
  x_2 \ge  \theta_0/\theta_2 + \theta_1/\theta_2x_1

$$

# Reference

All code and text came from Max Kuhn and Julia Silge`s book [Tidy Modeling with R](https://www.tmwr.org/tuning).