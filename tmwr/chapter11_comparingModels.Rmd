---
title: "Comparing Models with Resampling"
output: 
  md_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

# Comparing Models with Resampling

```{r}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)

lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(1003)
rf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)
```

## CREATING MULTIPLE MODELS WITH WORKFLOW SETS

Ltes create three different linear models that add different preprocessing steps incrementally, so we can test whether these additional terms impove the models results

```{r}
# three recipes

# basice one
basic_rec <- 
  recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
            Latitude + Longitude, data=ames_train) |> 
  step_log(Gr_Liv_Area, base=10) |> 
  step_other(Neighborhood, threshold=0.01) |> 
  step_dummy(all_nominal_predictors())

# with interaction terms
interaction_rec <- 
  basic_rec |> 
  step_interact(~Gr_Liv_Area:starts_with("Bldg_Type"))


# with splines
spline_rec <- 
  interaction_rec |> 
  step_ns(Latitude, Longitude, deg_free = 50)

basic_rec
interaction_rec
spline_rec

# using workflow set to create workflow with different recipes
lm_models <-
  workflow_set(
    preproc = list(
      basic = basic_rec,
      interact = interaction_rec,
      splines = spline_rec
    ),
    models = list(lm = linear_reg()),
    cross = F
  )
lm_models
```

We'd like to resample each of the models in turn.

```{r}
lm_models <-
  lm_models |>
  workflow_map(
    "fit_resamples",
    seed = 1101,
    verbose = T,
    resamples = ames_folds,
    control = keep_pred
  )
lm_models
```


```{r}
collect_metrics(lm_models) |> 
  filter(.metric=="rmse")
```

What about the random forest model?

```{r}
# get the workflow and converto to a workflow set, bind with the lms
four_models <- as_workflow_set(rand_forest = rf_res) |> 
  bind_rows(lm_models)

# it was fitted using the same ames_fold
four_models 
```

```{r}
autoplot(four_models)

#focusing in the Rˆ2 metric
library(ggrepel) 
autoplot(four_models, metric="rsq") +
  geom_text_repel(aes(label=wflow_id), 
                  nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")
```

## COMPARING RESAMPLED PERFORMANCE STATISTICS

Considering the preceding results for the three linear models, it appears that the additional terms do not profoundly improve the mean $RMSE$ or $Rˆ2$ statistics for the linear models. The difference is small, but it might be larger than the experimental noise in the system, i.e., considered statistically significant. We can formally test the hypothesis that the additional terms increase $Rˆ2$.

> Before making between-model comparisons, it is important for us to discuss the within-resample correlation for resampling statistics.

let’s gather the individual resampling statistics for the linear models and the random forest. We will focus on the $Rˆ2$ statistic for each mode

```{r}

rsq_indiv_estimates <-
  collect_metrics(four_models, summarize = FALSE) |>
  filter(.metric == "rsq")

rsq_wider <-
  rsq_indiv_estimates |>
  select(wflow_id, .estimate, id) |>
  pivot_wider(id_cols = id,
              names_from = wflow_id,
              values_from = .estimate)

corrr::correlate(rsq_wider |> 
                   select(-id)) |> 
  autoplot() +
  geom_text(aes(label=round(r,3)))

```

These correlations are high, and indicate that, across models, there are large within-resample correlations. To see this visually, the $Rˆ2$ statistics are shown for each model with lines connecting the resamples:

```{r}
rsq_indiv_estimates |> 
  mutate(wflow_id = reorder(wflow_id, .estimate)) |> 
  ggplot(aes(x=wflow_id, y=.estimate, group=id, color=id)) +
  geom_line(alpha=.5, linewidth=1.25) +
  theme_light() +
  theme(legend.position = "none") 
```

A statistical test for the correlations evaluates whether the magnitudes of these correlations are not simply noise. 

```{r}
rsq_wider |> 
  with( cor.test(basic_lm, splines_lm) ) |> 
  tidy() |> 
  select(estimate, starts_with("conf"))
```

The results of the correlation test (the estimate of the correlation and the confidence intervals) show us that the within-resample correlation appears to be real.

### ANOVA

#### One-way Comparation

We want to know if there is any significant difference between the average `.estimates` of $R^2$ in the different models.

```{r}
# one-way
rsq_indiv_estimates |> 
  select(wflow=wflow_id, id, value=.estimate) |> 
  aov(value ~ wflow, data =_) |> 
  summary()
```

As the `p-value` is less than the significance level 0.05, we can conclude that there are significant differences between the groups highlighted with “*" in the model summary.

#### Pair Wise Comparation

As the ANOVA test is significant, we can compute Tukey HSD (Tukey Honest Significant Differences, R function: `TukeyHSD()`) for performing multiple pairwise-comparison between the means of groups.

The function `TukeyHD()` takes the fitted ANOVA as an argument.

```{r}
rsq_indiv_estimates |> 
  select(wflow=wflow_id, id, value=.estimate) |> 
  aov(value ~ wflow, data =_) |>
  TukeyHSD() 
```

It can be seen from the output, that only the difference between `rand_forest` and `basic_lm` is significant with an adjusted `p-value` of 0.048.

```{r}
# using parwise t-test
pairwise.t.test(rsq_indiv_estimates$.estimate, 
                rsq_indiv_estimates$wflow_id, 
                p.adjust.method = "BH")
```

The result is a table of p-values for the pairwise comparisons. Here, the p-values have been adjusted by the Benjamini-Hochberg method.

#### Check for homoscadeasticity

The ANOVA test assumes that, the data are normally distributed and the variance across groups are homogeneous. We can check that with some diagnostic plots.

```{r}
par(mfrow=c(2,2))
rsq_indiv_estimates |> 
  select(wflow=wflow_id, id, value=.estimate) |> 
  aov(value ~ wflow, data =_) |> 
  plot()
par(mfrow=c(1,1))
```


## SIMPLE HYPOTHESIS TESTING METHODS

<!--

We can use simple hypothesis testing to make formal comparisons between models. Consider the familiar linear statistical model:

$$y_{ij}=\beta_{0} + \beta_{1}x_{i1} + \dots + \beta_{p}x_{ip} + \epsilon_{ij}$$

This versatile model is used to create regression models as well as being the basis for the popular analysis of variance (ANOVA) technique for comparing groups. With the ANOVA model, the predictors ($x_{ij}$) are binary dummy variables for different groups. From this, the $\beta$ parameters estimate whether two or more groups are different from one another using hypothesis testing techniques.

--> 

A simple and fast method for comparing two models at a time is to use the differences in $Rˆ2$ values as the outcome data in the ANOVA model. Since the outcomes are matched by resample, the differences do not contain the resample-to-resample effect and, for this reason, the standard ANOVA model is appropriate. To illustrate, this call to `lm()` tests the difference between two of the linear regression models:

```{r}
# using LM
compare_lm <- rsq_wider |> 
  mutate(difference=splines_lm - basic_lm)

lm(difference ~ 1, data=compare_lm) |> 
  tidy(conf.int = T) |> 
  select(estimate, p.value, starts_with("conf"))

# using a t.test
rsq_wider %>% 
  with( t.test(splines_lm, basic_lm, paired = TRUE) ) %>%
  tidy() %>% 
  select(estimate, p.value, starts_with("conf"))

```

We could evaluate each pair-wise difference in this way. Note that the p-value indicates a statistically significant signal; the collection of spline terms for longitude and latitude do appear to have an effect. However, the difference in $R^2$ is estimated at 0.91%. If our practical effect size were 2%, we might not consider these terms worth including in the model.

> `p-value`: “Informally, it is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.”

## Bayesian Methods

```{r}
library(tidyposterior)
library(rstanarm)

rsq_anova <- 
  perf_mod(
    four_models,
    metric="rsq",
    prior_intercept = rstanarm::student_t(df=1),
    chains=4,
    seed=1102
  )

# Take a random sample from the posterior distribution
# so set the seed again to be reproducible. 
rsq_anova |> 
  tidy(seed = 1103)

```

```{r}
rsq_anova |> 
  tidy(seed=1103) |> 
  mutate(model=forcats::fct_inorder(model)) |> 
  ggplot(aes(x=posterior)) +
  geom_histogram(bins=50, color="white", fill="blue", alpha=.6) +
  facet_wrap(~model, ncol=1) +
  theme_light()

```

These histograms describe the estimated probability distributions of the mean $R^2$ value for each model. There is some overlap, especially for the three linear models.

```{r}
rsq_anova |> 
  tidy(seed=1103) |> 
  autoplot() +
  theme_light()

rsq_anova |> 
  autoplot() +
  geom_text_repel(aes(label=workflow), nudge_x=1/8, nudge_y = 1/100) +
  theme_light()
  theme(legend.position = "nonee")

```

One wonderful aspect of using resampling with Bayesian models is that, once we have the posteriors for the parameters, it is trivial to get the posterior distributions for combinations of the parameters. 

```{r}
rsq_diff <- 
  contrast_models(rsq_anova,
                  list_1 = "splines_lm",
                  list_2 = "basic_lm",
                  seed=1104)

rsq_diff |> 
  as_tibble() |> 
  ggplot(aes(x=difference)) +
  geom_vline(xintercept = 0, lty=2) +
  geom_histogram(bins=50, color="white", fill="red", alpha=.6) +
  theme_light()
```

The `summary()` method for this object computes the mean of the distribution as well as credible intervals, the Bayesian analog to confidence intervals.

```{r}
summary(rsq_diff) |> 
  select(-starts_with("pract"))
```

The `probability` column reflects the proportion of the posterior that is greater than zero.

However, the estimate of the mean difference is fairly close to zero. Recall that the practical effect size we suggested previously is 2%. With a posterior distribution, we can also compute the probability of being practically significant. 

In Bayesian analysis, this is a *ROPE* estimate (for Region Of Practical Equivalence). To estimate this, the `size` option to the summary function is used:

```{r}
summary(rsq_diff, size=0.02) |> 
  select(contrast, starts_with("pract"))
```

The `pract_equiv` column is the proportion of the posterior that is within [-`size`, `size`] (the columns `pract_neg` and `pract_pos` are the proportions that are below and above this interval). 


The `autoplot()` method can show the `pract_equiv` results that compare each workflow to the current best (the random forest model, in this case).

```{r}
autoplot(rsq_anova, type="ROPE", size=0.02) +
  theme_light()
```

# Reference

All code and text came from Max Kuhn and Julia Silge`s book [Tidy Modeling with R](https://www.tmwr.org/compare).